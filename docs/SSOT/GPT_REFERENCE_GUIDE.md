# GPT Reference Guide - Gemantria Project Files

This guide explains the curated files available for GPT analysis of the Gemantria project. **Note**: For GPT PM context rebuild, see `docs/handoff/GPT_PM_CONTEXT_REBUILD.md` which consolidates project overview, history, and operational context.

## Core Project Documentation

**AGENTS.md** - Agent framework and operational contracts. Defines how automated agents work, their responsibilities, and governance rules.

**GEMATRIA_MASTER_REFERENCE.md** - Complete project master reference (1,183 lines). Contains all project details, architecture, agent contracts, rules, schemas, and operational procedures.

**MASTER_PLAN.md** - High-level project roadmap and goals. Strategic vision and phase planning.

**RULES_INDEX.md** - Complete index of all project rules. Governance framework with 61+ numbered rules for compliance.

## Configuration & Setup

**env_example.txt** - Environment variable template. Shows all required configuration variables.

**pyproject.toml** - Python project configuration. Dependencies, build settings, and tool configurations.

**pytest.ini** - Test configuration. How tests are run and configured.

**Makefile** - Build and automation targets. All available `make` commands for development, testing, and deployment.

## Schemas & Data Structures

**ai-nouns.schema.json** - Schema for AI-generated noun data. Defines structure of enriched concept data.

**graph.schema.json** - Schema for semantic network data. Defines node/edge structure for concept graphs.

**graph-patterns.schema.json** - Schema for graph pattern analysis. Community detection and centrality metrics.

**graph-stats.schema.json** - Schema for graph statistics. Network health and performance metrics.

**graph_stats.head.json** - Sample graph statistics data. Real example of graph analysis output.

**PMAGENT_REALITY_CHECK_AI_NOTES.md** - AI-generated orchestrator-facing notes about the pmagent reality-check system. Generated by `pmagent docs reality-check-ai-notes` using Granite (LM Studio) when available. Contains placeholder content when LM is offline.

## Documentation

**README.md** - Project overview and quick start. Basic introduction and setup instructions.

**README_FULL.md** - Comprehensive project documentation. Detailed guides and references.

**CHANGELOG.md** - Version history and changes. What changed in each release.

**RELEASES.md** - Release notes and deployment info. Version-specific release information.

## Development Workflow

**pull_request_template.md** - PR template requirements. What must be included in pull requests.

**pre-commit-config.yaml** - Code quality hooks. Automated checks that run before commits.

**SHARE_MANIFEST.json** - File sharing configuration. Defines which files are shared and how.

## Usage Guidelines for GPT

1. **Start with GEMATRIA_MASTER_REFERENCE.md** for complete project understanding
2. **Reference AGENTS.md** for operational procedures and agent contracts
3. **Check RULES_INDEX.md** for governance compliance requirements
4. **Use schemas** to understand data structures and validation rules
5. **Review Makefile** for available automation commands
6. **Check env_example.txt** for configuration requirements

## File Limits

This curated set of files stays within GPT's 22-file upload limit while providing comprehensive project understanding. The GPT PM Context Rebuild document (`docs/handoff/GPT_PM_CONTEXT_REBUILD.md`) consolidates several files (README, README_FULL, GEMATRIA_MASTER_REFERENCE) to reduce redundancy.

## GPT System Prompt Requirements

**Role Clarification:**
- **GPT = Project Manager (PM)**: Plans, decides, and provides instructions. Does NOT execute commands.
- **Cursor = Executor**: Reads GPT's instructions and runs the actual commands/tool calls.
- **Human = Orchestrator**: Coordinates the work, learns as we go, and needs clear explanations of what's happening and why.

**Session Initialization (MANDATORY):**
```bash
cd /home/mccoy/Projects/Gemantria.v2
source activate_venv.sh
make ssot.verify
```

**Validation Checklist:**
- ✅ Virtual environment active (.venv)
- ✅ Environment variables loaded
- ✅ Governance docs present (AGENTS.md, RULES_INDEX.md, GEMATRIA_MASTER_REFERENCE.md)
- ✅ Quality SSOT verified (ruff checks pass)
- ✅ Database accessible (PostgreSQL gematria and bible_db) - **Note**: Scripts handle DB unavailability gracefully (hermetic behavior per Rule 046)
- ✅ Share folder curated (files under 22-file GPT limit, see SHARE_MANIFEST.json for current count)

**Response Protocol (Two-Part Format):**
1. **Code Box for Cursor** (instructions Cursor will execute):
   - **Goal** — One sentence describing objective
   - **Commands** — Exact shell commands, top to bottom (for Cursor to execute)
   - **Evidence to return** — Which outputs Cursor should show
   - **Next gate** — What happens once evidence returned
2. **Tutor Notes** (outside the box, for the human orchestrator):
   - **Educational and explanatory**: Teach what's happening, not just summarize
   - **Define acronyms and terms**: Don't assume knowledge (e.g., "DSN = Database connection string")
   - **Explain WHY, not just WHAT**: Help the orchestrator understand the reasoning
   - **Plain English**: Avoid jargon; if you must use technical terms, explain them
   - **Help them learn**: The orchestrator knows enough to break things; guide them safely

**Tool Priority (for Cursor to use):**
1. local+gh (git, make, gh pr)
2. codex (if available, else "Codex disabled (401)")
3. gemini/mcp (for long docs)

## Hermetic Behavior (DB/Service Availability)

**Rule 046**: Hermetic CI Fallbacks - Scripts must handle missing/unavailable databases gracefully:
- DB-dependent operations check availability first
- Emit HINTs (not errors) when DB unavailable
- Housekeeping passes even when DB unavailable
- Per AGENTS.md: "If DB/services down → 'correct hermetic behavior.'"

**Example**: `governance_tracker.py` checks DB availability, emits HINTs, and returns success when DB unavailable, allowing `make housekeeping` to pass.

## Local Model Configuration (LM Studio & Ollama)

This system supports multiple local inference providers controlled by `INFERENCE_PROVIDER`:

- `lmstudio` – Use LM Studio's OpenAI-compatible API (`OPENAI_BASE_URL`).
- `ollama` – Use Ollama's native HTTP API (`OLLAMA_BASE_URL`).

The following environment variables control how local models are selected:

- **`INFERENCE_PROVIDER`** – `lmstudio` (default) or `ollama`.
- **`OPENAI_BASE_URL`** – Base URL for LM Studio's OpenAI-compatible API.
- **`OLLAMA_BASE_URL`** – Base URL for Ollama's API (default: `http://127.0.0.1:11434`).
- **`EMBEDDING_MODEL`** – Vector model (default: `text-embedding-bge-m3`).
- **`THEOLOGY_MODEL`** – Main reasoning/theology model.
- **`LOCAL_AGENT_MODEL`** – Local agent model (e.g. Granite 4 Tiny-H - available in both LM Studio and Ollama).
- **`MATH_MODEL`** – Optional math-heavy model.
- **`RERANKER_MODEL`** – Optional reranker model (Granite reranker planned later).
- **`RETRIEVAL_PROFILE`** – `LEGACY` (default) keeps BGE + Qwen retrieval. `GRANITE` switches retrieval embeddings/rerankers to Granite IDs using `GRANITE_EMBEDDING_MODEL`, `GRANITE_RERANKER_MODEL`, and `GRANITE_LOCAL_AGENT_MODEL`. Missing Granite overrides trigger a HINT and fall back to LEGACY.
- **`AUTO_START_MCP_SSE`** – Auto-start MCP SSE server during bring-up (default: `0`).

All LM configuration is centralized in `scripts/config/env.py` via `get_lm_model_config()`. The LM Studio adapter and the Ollama adapter both read from the same configuration and pick the correct runtime based on `INFERENCE_PROVIDER`.

**Phase-7E Profiles:**
- **LEGACY**: Current working setup (BGE + Qwen models) - see `env_example.txt` for configuration
- **GRANITE**: Recommended Granite-based setup (Phase-7E) - available in both LM Studio and Ollama:
  - **LM Studio**: Use `INFERENCE_PROVIDER=lmstudio` and install via `lms get granite` (see `docs/runbooks/LM_STUDIO_SETUP.md`)
  - **Ollama**: Use `INFERENCE_PROVIDER=ollama` and `ollama pull ibm/granite4.0-preview:tiny`

**Legacy Support (Deprecated):**
- `LM_EMBED_MODEL` → Use `EMBEDDING_MODEL` instead (will be removed in Phase-8)
- `QWEN_RERANKER_MODEL` → Use `RERANKER_MODEL` instead (will be removed in Phase-8)

### LM Studio Model Discovery

Use the discovery helper to list available LM Studio models and validate configuration:

```bash
python -m scripts.lm_models_ls
```

This command calls the LM Studio `/v1/models` endpoint (via `OPENAI_BASE_URL`) and verifies that `EMBEDDING_MODEL`, `THEOLOGY_MODEL`, `LOCAL_AGENT_MODEL`, and `RERANKER_MODEL` exist.

### LM Model Catalog (Phase-7D)

The complete model catalog is maintained in `docs/SSOT/LM_MODEL_CATALOG.json`. This catalog includes:
- **LEGACY profile**: Current working setup (BGE + Qwen models)
- **GRANITE profile**: Recommended Granite-based setup (Phase-7D)

Each model entry includes:
- Model ID (as it appears in LM Studio)
- Type (chat, embedding, reranker)
- Size information
- Installation status
- Download commands (for Granite models)

To install Granite models, use the interactive CLI:
```bash
# Interactive installation (recommended)
lms get granite
# Then select: ibm-granite/granite-4.0-h-tiny-GGUF

# Or use the helper script
./scripts/lm_install_granite.sh
```

See `docs/runbooks/LM_STUDIO_SETUP.md` for complete installation instructions.

### LM Studio Adapter (Phase-7D)

The LM Studio adapter (`agentpm/adapters/lm_studio.py`) is aligned with the canonical model loader (`get_lm_model_config()`). The adapter supports model slots:

- **`model_slot="theology"`** – Uses `THEOLOGY_MODEL` from config
- **`model_slot="local_agent"`** – Uses `LOCAL_AGENT_MODEL` from config
- **`model_slot="math"`** – Uses `MATH_MODEL` from config
- **`model_slot=None`** – Legacy fallback (uses `LM_STUDIO_MODEL` if set)

Example usage:
```python
from agentpm.adapters.lm_studio import lm_studio_chat

# Use theology model
result = lm_studio_chat(
    messages=[{"role": "user", "content": "..."}],
    model_slot="theology"
)

# Use local agent model
result = lm_studio_chat(
    messages=[{"role": "user", "content": "..."}],
    model_slot="local_agent"
)
```

### OPS Command Ledger (v0)

Successful OPS command bundles can be recorded in the OPS Command Ledger:

- File: `share/ops_command_ledger.jsonl`
- Helper: `scripts/ops_ledger.append_entry()`

Future phases may mine this ledger for reusable command sequences.

## Governance Reference

**ADR-058**: GPT System Prompt Requirements as Operational Governance - Establishes GPT system prompt requirements as part of the operational governance framework.

**Rule 046**: Hermetic CI Fallbacks - Defines graceful handling of unavailable services/databases.
